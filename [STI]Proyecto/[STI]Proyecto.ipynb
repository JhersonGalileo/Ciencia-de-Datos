{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "miniature-somerset",
   "metadata": {},
   "source": [
    "## **Proyecto STI **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import OrdinalEncoder, normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_titanic_proyecto.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validamos cuantas columnas tienen registros NaN\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(df, column):\n",
    "    cond =  data[column].isna()\n",
    "    data.loc[ cond, column] = data[column].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan_percentage(df,column):\n",
    "    return(df[column].isna().sum()/len(df[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizamos el procentaje de Nan de la variable Age\n",
    "print(count_nan_percentage(data,\"Age\"))\n",
    "#Al tener un porcentaje manejable vamos a realizar imputacion de mediana\n",
    "remove_nan(data,\"Age\")\n",
    "print(\"Porcentaje de NANs\",count_nan_percentage(data,\"Age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se valida el porcentaje de NANs de Cabin\n",
    "print(data[\"Cabin\"].isna().sum()/len(data[\"Cabin\"]))\n",
    "#como el procentaje de datos NANs es demasiado alto se procede a eliminar esta variable del data set\n",
    "data = data.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambien vemos que existen dos registros de la variable Embarked, al ser de tipo no numerico \n",
    "#se procede a eliminar estos registros\n",
    "cond = data[\"Embarked\"].notna()\n",
    "#data = data[cond]\n",
    "data = data.loc[cond]\n",
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_corr_matrix(data):\n",
    "    corrMatrix = data.corr()\n",
    "    print(corrMatrix)\n",
    "    sn.heatmap(corrMatrix, annot=True)\n",
    "    plt.show()\n",
    "    return corrMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tambien se elimina las variables categoricas id,Name,y Ticket que son datos con caracteristicas especificas\n",
    "data = data.drop(['PassengerId','Name','Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = graph_corr_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ordinal_encoding(vec_x):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    vec_aux = np.asarray(vec_x.unique())\n",
    "    #print(vec_aux)\n",
    "    vec_x = vec_x.values\n",
    "    #print(vec_x)\n",
    "    le.fit(vec_aux)\n",
    "    return le.transform(vec_x)\n",
    "    \n",
    "    #enc = OrdinalEncoder()\n",
    "    #enc.fit(vec_x)\n",
    "    #OrdinalEncoder()\n",
    "    #return enc.transform(vec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked']           = return_ordinal_encoding(data['Embarked'])\n",
    "data['passenger_class']    = return_ordinal_encoding(data['passenger_class'])\n",
    "data['passenger_sex']      = return_ordinal_encoding(data['passenger_sex'])\n",
    "data['passenger_survived'] = return_ordinal_encoding(data['passenger_survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-reverse",
   "metadata": {},
   "source": [
    "**Separamos la informacion de forma aleatoria Train-val-test Split con scikit learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = list(data.columns)[: -1]\n",
    "y_var = list(data.columns)[-1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[x_vars], data[y_var], test_size=0.20, random_state=42)\n",
    "print(\"Compruebo sumas iguales\",len(X_train)+len(X_test),data.shape[0])\n",
    "X_train, X_val, Y_train,  Y_val  = train_test_split(X_train, Y_train, test_size=0.20, random_state=42)\n",
    "print(\"Compruebo sumas iguales\",len(X_train)+len(X_val)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decision_tree(x_train,y_train,x_test, max_depth=None,critery=\"entropy\"):\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(criterion=critery, max_depth=max_depth)\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(x_train,y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_test, y_pred):\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba 1 de el arbol de decision\n",
    "model, y_pred = training_decision_tree(X_train, Y_train,X_val,3)\n",
    "calculate_accuracy(y_pred,Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_tree(clf, x_vars,file_name):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = x_vars,class_names=['0','1'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png(file_name)\n",
    "    Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_tree(model,x_vars,'arbol.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('arbol.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-decimal",
   "metadata": {},
   "source": [
    "## **Entrenamiento con SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(x_train, y_train, x_test, kernel =\"linear\", c = 10):\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel=kernel, C=c) # Linear Kernel\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(x_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_pred = train_svm(X_train, Y_train,X_val,\"poly\",10000)\n",
    "calculate_accuracy(y_pred,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-recruitment",
   "metadata": {},
   "source": [
    "## **Entrenamiento para algoritmo NAIVE BAYES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(x_train,y_train, x_test):\n",
    "    x_not_surv = x_train[y_train == 0]\n",
    "    x_survided = x_train[y_train == 1]\n",
    "    \n",
    "    #Estadisticos requeridos\n",
    "    \n",
    "    #media\n",
    "    mean_survided = np.mean(x_survided)\n",
    "    mean_not_surv  = np.mean(x_not_surv) \n",
    "    \n",
    "    #desviacion estandar\n",
    "    sd_survided = np.std(x_survided)\n",
    "    sd_not_surv  = np.std(x_not_surv) \n",
    "    \n",
    "    stadistic_survided = np.stack((mean_survided,sd_survided)).T\n",
    "    stadistic_not_surv = np.stack((mean_not_surv,sd_not_surv)).T\n",
    "    \n",
    "    #print(stadistic_survided)\n",
    "    #print(stadistic_not_surv)\n",
    "    \n",
    "    #print((sd_not_surv))\n",
    "    \n",
    "    prob_class_0 = (1-np.mean(Y_train))\n",
    "    prob_class_1 = np.mean(Y_train)\n",
    "\n",
    "    summaries = dict()\n",
    "    summaries['0'] = (stadistic_not_surv,prob_class_0)\n",
    "    summaries['1'] = (stadistic_survided,prob_class_1)\n",
    "    print(len(summaries))\n",
    "\n",
    "    probabilities=dict()\n",
    "    \n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = class_summaries[1]\n",
    "        class_summaries = class_summaries[0]\n",
    "        for i in range(len(class_summaries)):\n",
    "            print(\"I va por\",class_summaries[i])\n",
    "            #mean, stdev = class_summaries[i]\n",
    "            #probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-rapid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_naive_bayes(X_train, Y_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-season",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = np.exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (np.sqrt(2 * np.pi) * stdev)) * exponent\n",
    "\n",
    "total_rows = len(X_train)\n",
    "\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    row = row[0] \n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] =class_summaries[1]/float(total_rows)\n",
    "        class_summaries = class_summaries[0]\n",
    "        for i in range(len(class_summaries)):\n",
    "            #print(\"iter:\",i,\"Variable:\",row[i],\"Estadisticos:\",class_summaries[i])\n",
    "            mean, stdev = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "    \n",
    "\n",
    "    \n",
    "total_y0_rows = len(X_train[Y_train == 0])\n",
    "total_y1_rows = len(X_train[Y_train == 1])\n",
    "\n",
    "prob_1 =np.asarray([[28.04509091, 13.30357469],\n",
    "                    [ 0.47272727,  0.72248424],\n",
    "                    [ 0.40909091 , 0.72357012],\n",
    "                    [48.60174318 ,68.99254371],\n",
    "                    [ 1.36818182 , 0.87172054],\n",
    "                    [ 1.02727273 , 0.86296625],\n",
    "                    [ 0.32272727 , 0.46751939]]\n",
    "                  )\n",
    "\n",
    "prob_2 =np.array([[30.04454023, 12.6258105 ],\n",
    "                    [ 0.62643678 , 1.32117675],\n",
    "                    [ 0.37643678 , 0.88031927],\n",
    "                    [23.22605259 ,33.08035429],\n",
    "                    [ 1.63505747 , 0.70843096],\n",
    "                    [ 0.44827586 , 0.73513812],\n",
    "                    [ 0.85057471 , 0.35650718]]\n",
    "                  )\n",
    "\n",
    "def predict(summaries, x_train):\n",
    "    #labels = list()\n",
    "    probabilities = calculate_class_probabilities(summaries, x_train)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    #list.append(bestLabel)\n",
    "    \n",
    "    return bestLabel#labels\n",
    "\n",
    "\n",
    "summaries = dict()\n",
    "summaries['0'] = (prob_1,total_y0_rows)\n",
    "summaries['1'] = (prob_2,total_y1_rows)\n",
    "\n",
    "df = X_train.copy()\n",
    "#print(df.head())\n",
    "x_1 =df.iloc[[0]] \n",
    "#print(x_1.values)\n",
    "#df['race_label'] = df.apply (lambda row: label_race(row), axis=1)\n",
    "#print(X_train.apply (lambda row: predict(summaries,row), axis=1))\n",
    "\n",
    "for i in range(15):\n",
    "    print(predict(summaries,X_train.iloc[[i]].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-mounting",
   "metadata": {},
   "source": [
    "## Algoritmo de Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_reg(X_test, weights):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    tensor_w = tf.placeholder(tf.float32, shape = [None, None], name = \"tensor_w\")\n",
    "    tensor_x = tf.placeholder(tf.float32, shape = [None, None], name = \"tensor_x\")\n",
    "    \n",
    "    logits = tf.matmul(tensor_x, tf.transpose(tensor_w))\n",
    "    sigmoid = tf.nn.sigmoid(logits)\n",
    "    prediction = tf.round(sigmoid)\n",
    "    \n",
    "    with tf.train.MonitoredSession() as session:\n",
    "        feed_dictionary = {tensor_x: X_test, tensor_w:weights}\n",
    "        \n",
    "        predictions = session.run(prediction, feed_dict= feed_dictionary)\n",
    "        \n",
    "        return predictions.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    \n",
    "    def __init__(self, x,  lambda_parameter, regularization):\n",
    "        # vector de weights para logits\n",
    "        self.w = tf.get_variable(\"weights\", dtype = tf.float32, shape = [1,x.shape[1]], initializer = tf.zeros_initializer())\n",
    "        self.lambda_parameter = tf.constant(lambda_parameter, dtype = tf.float32)\n",
    "        self.regularization = regularization\n",
    "    \n",
    "    # Funcion para el calculo de logits\n",
    "    def logits(self, x):\n",
    "        with tf.name_scope(\"logits\"):\n",
    "            return tf.matmul(x, tf.transpose(self.w))\n",
    " \n",
    "    def prediction(self, x):\n",
    "        #Method to produce probability distribution for output\n",
    "        sigmoid = tf.nn.sigmoid(self.logits(x))\n",
    "        predictions = tf.round(sigmoid)\n",
    "        return predictions\n",
    "    \n",
    "    def accuracy(self, y_hat, y):\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.equal(y_hat, y)\n",
    "            accuracy = tf.cast(accuracy, tf.int32)\n",
    "            accuracy = tf.divide(tf.reduce_sum(accuracy), tf.shape(y)[0])\n",
    "            return accuracy \n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = self.logits(x)))\n",
    "        if self.regularization == 'lasso':\n",
    "            error = tf.add(cross_entropy,tf.multiply(self.lambda_parameter, tf.reduce_sum(tf.abs(self.w))))\n",
    "        if self.regularization == 'ridge':\n",
    "            error = tf.add(cross_entropy, tf.multiply(self.lambda_parameter, tf.sqrt(tf.reduce_sum(tf.square(self.w)))))\n",
    "        return error\n",
    "    \n",
    "    def update(self, x_train, y_train, x_test, y_test, learningrate):\n",
    "        with tf.name_scope(\"error\"):\n",
    "            train_error = self.loss(x_train, y_train)\n",
    "            train_error_summary = tf.summary.scalar(\"TrainError\", train_error)\n",
    "            test_error = self.loss(x_test, y_test)\n",
    "            test_error_summary = tf.summary.scalar(\"ValError\", test_error)\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            train_accuracy = self.accuracy(self.prediction(x_train), y_train)\n",
    "            train_accuracy_summary = tf.summary.scalar(\"TrainAccuracy\", train_accuracy)\n",
    "            test_accuracy = self.accuracy(self.prediction(x_test), y_test)\n",
    "            test_accuracy_summary = tf.summary.scalar(\"ValAccuracy\", test_accuracy)\n",
    "        # update parameters via gradient descent\n",
    "        gradient = tf.gradients(train_error, [self.w])\n",
    "        updated_w = tf.assign(self.w, self.w - learningrate * gradient[0])\n",
    "        \n",
    "        #return updated_w, train_error, train_accuracy\n",
    "        \n",
    "        return updated_w, train_error, test_error, train_accuracy, test_accuracy, train_error_summary, \\\n",
    "        test_error_summary, train_accuracy_summary, test_accuracy_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LogisticRegressionClass(lr, epochs, frecprint, x_train, x_test, labels_train, labels_test, batch_size, lambda_parameter, regularization):\n",
    "    # String para definicion de experimento\n",
    "    string = './experiments/LogRegression'+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +\"_lr=\"+str(lr)+ \"_epochs=\"+str(epochs) + \"_batchsize=\" \\\n",
    "    + str(batch_size) + '_lambda_parameters=' + str(lambda_parameter) + \"_regularization=\" + regularization\n",
    "\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        # Inicialización de objeto de clase LogisticRegressionSoftmax\n",
    "        modelo = LogisticRegressionSigmoid(x_train, lambda_parameter, regularization)\n",
    "        # Definicion de placeholders para input de data a grafo\n",
    "        with tf.name_scope(\"train_tensors\"):\n",
    "            tensor_x_train = tf.placeholder(tf.float32, [None,x_train.shape[1]], \"tensor_x_train\")\n",
    "            tensor_labels_train = tf.placeholder(tf.float32, [None,labels_train.shape[1]], \"tensor_labels_train\")\n",
    "        with tf.name_scope(\"val_tensors\"):\n",
    "            tensor_x_test = tf.placeholder(tf.float32, [x_test.shape[0], x_test.shape[1]], \"tensor_x_val\")\n",
    "            tensor_labels_test = tf.placeholder(tf.float32, [labels_test.shape[0], labels_test.shape[1]], \"tensor_labels_val\")\n",
    "        # Utilizacion de método update para hacer el entrenamiento\n",
    "        update_parameters = modelo.update(tensor_x_train, tensor_labels_train, tensor_x_test, tensor_labels_test, lr)\n",
    "    \n",
    "        # Writer para utilizacion de tensorboard\n",
    "        writer = tf.summary.FileWriter(string, g)\n",
    "        \n",
    "        #batch_size = 32\n",
    "        \n",
    "        total_steps = int((labels_train.shape[0] / batch_size) * epochs)  \n",
    "        \n",
    "        with tf.train.MonitoredSession() as session:\n",
    "            \n",
    "            for i in range(total_steps + 1):\n",
    "                # Mini batch gradient descent with batch size 32\n",
    "                offset = (i * batch_size) % (labels_train.shape[0] - batch_size)\n",
    "                batch_data = x_train[offset:(offset + batch_size),]\n",
    "                batch_labels = labels_train[offset:(offset + batch_size),]\n",
    "                feed_dict = {tensor_x_train:batch_data, tensor_labels_train:batch_labels,tensor_x_test:x_test, tensor_labels_test:labels_test}\n",
    "                # Entrenamiento\n",
    "                training = session.run(update_parameters, feed_dict = feed_dict)\n",
    "                if (i)%frecprint == 0:\n",
    "                    # Cálculo de pesos para print por cada frecprint epocas\n",
    "                    weights = session.run(modelo.w, feed_dict = feed_dict)\n",
    "                    # Agregar datos a writer para poder visualizarlos en tensorboard\n",
    "                    writer.add_summary(training[5], i)\n",
    "                    writer.add_summary(training[6], i)\n",
    "                    writer.add_summary(training[7], i)\n",
    "                    writer.add_summary(training[8], i)\n",
    "                    print(\"Mini-batch:\", i, \"train error:\", training[1], \"train accuracy:\", training[3])\n",
    "                    print(\"Epoch:\", int(i//(labels_train.shape[0] / batch_size)+1), \"validation error:\", training[2], \"validation accuracy:\", training[4])\n",
    "                    print(\"-------------------------------------------------------------------------\")\n",
    "            \n",
    "            return weights\n",
    "\n",
    "            writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
